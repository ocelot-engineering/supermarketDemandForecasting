{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ETL for main data\n",
    "The following script builds each of the datasets provided by Kaggle.\n",
    "\n",
    "While this script may seem to have a lot of repetition, many of the functions used have been extracted moved to the `src/utils/etl_utils`. The assumption is that each dataset will require individual treatment, and this will become more clear the data is better understood. Repeating each file cell by cell gives a data engineer a lot more freedom to make changes to individual datasets rather than treat them all the same.\n",
    "\n",
    "Note, that the schemas deliberately coupled with the etl cells below rather than living externally. This is because it is very unlikely a dataset will have multiple different schemas, hence simplifying in this instance by coupling configuration and functionality."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clear the interim data directory\n",
    "This should be run when an entire refresh is required."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.utils.etl_utils import *\n",
    "\n",
    "refresh = True # set to True if refresh of all data will be done.\n",
    "\n",
    "if refresh:\n",
    "    clear_interm_data_dir()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decompress downloaded file\n",
    "(main file from Kaggle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['items.csv.7z', 'transactions.csv.7z', 'holidays_events.csv.7z', 'train.csv.7z', 'stores.csv.7z', 'oil.csv.7z', 'test.csv.7z', 'sample_submission.csv.7z']\n"
     ]
    }
   ],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "\n",
    "decomp_file(file_path=proj.Config.paths.get(\"data_raw\").joinpath('favorita-grocery-sales-forecasting.zip'),\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"))\n",
    "\n",
    "file_list = get_file_list(dir_path=proj.Config.paths.get(\"data_interim\"),\n",
    "                          regex_etx_pattern=\".*\\\\.7z$\")\n",
    "\n",
    "print(file_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare extracted data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Holidays Events\n",
    "Holidays and Events, with metadata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "from re import sub\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "file_name = \"holidays_events.csv.7z\"\n",
    "\n",
    "# Decompress file\n",
    "file_path = proj.Config.paths.get(\"data_interim\").joinpath(file_name)\n",
    "decomp_file(file_path=file_path,\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"),\n",
    "            print_msg=True)\n",
    "\n",
    "# Convert decompressed csv to parquet file with custom schema\n",
    "file_path_csv = Path(sub(\"\\\\.7z$\", \"\", str(file_path)))\n",
    "parquet_file_name = Path(sub(\"\\\\.csv\\\\.7z$\", \".parquet\", str(file_name)))\n",
    "parquet_file_path = proj.Config.paths.get(\"data_proc\").joinpath(parquet_file_name)\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add('date', DateType(), True)\\\n",
    "    .add('type', StringType(), True)\\\n",
    "    .add('locale', StringType(), True)\\\n",
    "    .add('locale_name', StringType(), True)\\\n",
    "    .add('description', StringType(), True)\\\n",
    "    .add('transferred', BooleanType(), True)\n",
    "\n",
    "csv_to_parquet(read_path=file_path_csv,\n",
    "               write_path=parquet_file_path,\n",
    "               schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Items\n",
    "Item metadata, including family, class, and perishable.\n",
    "NOTE: Items marked as perishable have a score weight of 1.25; otherwise, the weight is 1.0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "from re import sub\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "file_name = \"items.csv.7z\"\n",
    "\n",
    "# Decompress file\n",
    "file_path = proj.Config.paths.get(\"data_interim\").joinpath(file_name)\n",
    "decomp_file(file_path=file_path,\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"),\n",
    "            print_msg=True)\n",
    "\n",
    "# Convert decompressed csv to parquet file with custom schema\n",
    "file_path_csv = Path(sub(\"\\\\.7z$\", \"\", str(file_path)))\n",
    "parquet_file_name = Path(sub(\"\\\\.csv\\\\.7z$\", \".parquet\", str(file_name)))\n",
    "parquet_file_path = proj.Config.paths.get(\"data_proc\").joinpath(parquet_file_name)\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add('item_nbr', IntegerType(), True)\\\n",
    "    .add('family', StringType(), True)\\\n",
    "    .add('class', StringType(), True)\\\n",
    "    .add('perishable', IntegerType(), True)\n",
    "\n",
    "csv_to_parquet(read_path=file_path_csv,\n",
    "               write_path=parquet_file_path,\n",
    "               schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Oil\n",
    "Daily oil price. Includes values during both the train and test data timeframe. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)\n",
    "But very unlikely this data will be useful for the model unless we were to know future oil prices (which we would need to predict)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "from re import sub\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "file_name = \"oil.csv.7z\"\n",
    "\n",
    "# Decompress file\n",
    "file_path = proj.Config.paths.get(\"data_interim\").joinpath(file_name)\n",
    "decomp_file(file_path=file_path,\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"),\n",
    "            print_msg=True)\n",
    "\n",
    "# Convert decompressed csv to parquet file with custom schema\n",
    "file_path_csv = Path(sub(\"\\\\.7z$\", \"\", str(file_path)))\n",
    "parquet_file_name = Path(sub(\"\\\\.csv\\\\.7z$\", \".parquet\", str(file_name)))\n",
    "parquet_file_path = proj.Config.paths.get(\"data_proc\").joinpath(parquet_file_name)\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add('date', DateType(), True)\\\n",
    "    .add('dcoilwtico', FloatType(), True)\n",
    "\n",
    "csv_to_parquet(read_path=file_path_csv,\n",
    "               write_path=parquet_file_path,\n",
    "               schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample submission\n",
    "A sample submission file in the correct format.\n",
    "\n",
    "Note: this is not input data, but instead an illustration of the structure of a submission to Kaggle.\n",
    "Notice ID is included here, meaning we must continue to keep ID for the test data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "from re import sub\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "file_name = \"sample_submission.csv.7z\"\n",
    "\n",
    "# Decompress file\n",
    "file_path = proj.Config.paths.get(\"data_interim\").joinpath(file_name)\n",
    "decomp_file(file_path=file_path,\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"),\n",
    "            print_msg=True)\n",
    "\n",
    "# Convert decompressed csv to parquet file with custom schema\n",
    "file_path_csv = Path(sub(\"\\\\.7z$\", \"\", str(file_path)))\n",
    "parquet_file_name = Path(sub(\"\\\\.csv\\\\.7z$\", \".parquet\", str(file_name)))\n",
    "parquet_file_path = proj.Config.paths.get(\"data_proc\").joinpath(parquet_file_name)\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add('id', IntegerType(), True)\\\n",
    "    .add('unit_sales', FloatType(), True)\n",
    "\n",
    "csv_to_parquet(read_path=file_path_csv,\n",
    "               write_path=parquet_file_path,\n",
    "               schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stores\n",
    "Store metadata, including city, state, type, and cluster.\n",
    "cluster is a grouping of similar stores."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "from re import sub\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "file_name = \"stores.csv.7z\"\n",
    "\n",
    "# Decompress file\n",
    "file_path = proj.Config.paths.get(\"data_interim\").joinpath(file_name)\n",
    "decomp_file(file_path=file_path,\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"),\n",
    "            print_msg=True)\n",
    "\n",
    "# Convert decompressed csv to parquet file with custom schema\n",
    "file_path_csv = Path(sub(\"\\\\.7z$\", \"\", str(file_path)))\n",
    "parquet_file_name = Path(sub(\"\\\\.csv\\\\.7z$\", \".parquet\", str(file_name)))\n",
    "parquet_file_path = proj.Config.paths.get(\"data_proc\").joinpath(parquet_file_name)\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('city', StringType(), True)\\\n",
    "    .add('state', StringType(), True)\\\n",
    "    .add('type', StringType(), True)\\\n",
    "    .add('cluster', StringType(), True) # is an integer, but given it's a grouping better to keep it categorical\n",
    "\n",
    "csv_to_parquet(read_path=file_path_csv,\n",
    "               write_path=parquet_file_path,\n",
    "               schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test\n",
    "Test data, with the date, store_nbr, item_nbr combinations that are to be predicted, along with the onpromotion information.\n",
    "NOTE: The test data has a small number of items that are not contained in the training data. Part of the exercise will be to predict a new item sales based on similar products.\n",
    "The public / private leaderboard split is based on time. All items in the public split are also included in the private split."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "from re import sub\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "file_name = \"test.csv.7z\"\n",
    "\n",
    "# Decompress file\n",
    "file_path = proj.Config.paths.get(\"data_interim\").joinpath(file_name)\n",
    "decomp_file(file_path=file_path,\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"),\n",
    "            print_msg=True)\n",
    "\n",
    "# Convert decompressed csv to parquet file with custom schema\n",
    "file_path_csv = Path(sub(\"\\\\.7z$\", \"\", str(file_path)))\n",
    "parquet_file_name = Path(sub(\"\\\\.csv\\\\.7z$\", \".parquet\", str(file_name)))\n",
    "parquet_file_path = proj.Config.paths.get(\"data_proc\").joinpath(parquet_file_name)\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add('id', IntegerType(), True) \\\n",
    "    .add('date', DateType(), True)\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('item_nbr', IntegerType(), True)\\\n",
    "    .add('onpromotion', BooleanType(), True)\n",
    "\n",
    "csv_to_parquet(read_path=file_path_csv,\n",
    "               write_path=parquet_file_path,\n",
    "               schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train\n",
    "Training data, which includes the target unit_sales by date, store_nbr, and item_nbr and a unique id to label rows.\n",
    "The target unit_sales can be integer (e.g., a bag of chips) or float (e.g., 1.5 kg of cheese).\n",
    "Negative values of unit_sales represent returns of that particular item.\n",
    "The onpromotion column tells whether that item_nbr was on promotion for a specified date and store_nbr.\n",
    "Approximately 16% of the onpromotion values in this file are NaN.\n",
    "NOTE: The training data does not include rows for items that had zero unit_sales for a store/date combination. There is no information as to whether or not the item was in stock for the store on the date, and teams will need to decide the best way to handle that situation. Also, there are a small number of items seen in the training data that aren't seen in the test data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "from re import sub\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "file_name = \"train.csv.7z\"\n",
    "\n",
    "# Decompress file\n",
    "file_path = proj.Config.paths.get(\"data_interim\").joinpath(file_name)\n",
    "decomp_file(file_path=file_path,\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"),\n",
    "            print_msg=True)\n",
    "\n",
    "# Convert decompressed csv to parquet file with custom schema\n",
    "file_path_csv = Path(sub(\"\\\\.7z$\", \"\", str(file_path)))\n",
    "parquet_file_name = Path(sub(\"\\\\.csv\\\\.7z$\", \".parquet\", str(file_name)))\n",
    "parquet_file_path = proj.Config.paths.get(\"data_proc\").joinpath(parquet_file_name)\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add('id', IntegerType(), True) \\\n",
    "    .add('date', DateType(), True)\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('item_nbr', IntegerType(), True)\\\n",
    "    .add('unit_sales', FloatType(), True)\\\n",
    "    .add('onpromotion', BooleanType(), True)\n",
    "\n",
    "read_path=file_path_csv\n",
    "write_path=parquet_file_path\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv(str(read_path), header=True, schema=schema)\n",
    "df = df.drop(\"id\") # ID will never be used for this data set, best to save space\n",
    "df.write.parquet(str(write_path), mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transactions\n",
    "The count of sales transactions for each date, store_nbr combination. Only included for the training data timeframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from src.utils.etl_utils import *\n",
    "from re import sub\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "file_name = \"transactions.csv.7z\"\n",
    "\n",
    "# Decompress file\n",
    "file_path = proj.Config.paths.get(\"data_interim\").joinpath(file_name)\n",
    "decomp_file(file_path=file_path,\n",
    "            extract_path=proj.Config.paths.get(\"data_interim\"),\n",
    "            print_msg=True)\n",
    "\n",
    "# Convert decompressed csv to parquet file with custom schema\n",
    "file_path_csv = Path(sub(\"\\\\.7z$\", \"\", str(file_path)))\n",
    "parquet_file_name = Path(sub(\"\\\\.csv\\\\.7z$\", \".parquet\", str(file_name)))\n",
    "parquet_file_path = proj.Config.paths.get(\"data_proc\").joinpath(parquet_file_name)\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add('date', DateType(), True)\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('transactions', IntegerType(), True)\n",
    "\n",
    "\n",
    "csv_to_parquet(read_path=file_path_csv,\n",
    "               write_path=parquet_file_path,\n",
    "               schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean up interim directory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clear_interm_data_dir()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}