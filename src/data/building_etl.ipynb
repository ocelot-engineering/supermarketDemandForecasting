{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploring how ETL will be done.\n",
    "This notebook will investigate the viability of transforming the data into parquet files using Spark.\n",
    "Pros with using Parquet over csv are that it has an embedded schema, takes up less storage and can extract individual columns much faster.\n",
    "\n",
    "Note that **this notebook is not an EDA**. The goal is for investigating ETL design only therefore EDA is out of scope.\n",
    "\n",
    "This notebook will: see what data we have, understand the schemas and determine how to move forward with the ETL script."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discover what datasets we have.\n",
    "Start by defining some folder paths and looking at the downloaded data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "import py7zr # need for decompression of 7z\n",
    "import time # for timing execution\n",
    "\n",
    "PATH_RAW_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('raw')\n",
    "PATH_INTERIM_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('interim')\n",
    "PATH_PROC_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('processed')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "favorita-grocery-sales-forecasting.zip\n"
     ]
    }
   ],
   "source": [
    "# Get compressed file name\n",
    "print(os.listdir(PATH_RAW_DATA_DIR)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "PATH_RAW_DATA = PATH_RAW_DATA_DIR.joinpath('favorita-grocery-sales-forecasting.zip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ZipInfo filename='holidays_events.csv.7z' compress_type=deflate file_size=1898 compress_size=1903>\n",
      "<ZipInfo filename='items.csv.7z' compress_type=deflate file_size=14315 compress_size=14320>\n",
      "<ZipInfo filename='oil.csv.7z' compress_type=deflate file_size=3762 compress_size=3767>\n",
      "<ZipInfo filename='sample_submission.csv.7z' compress_type=deflate file_size=666528 compress_size=649511>\n",
      "<ZipInfo filename='stores.csv.7z' compress_type=deflate file_size=648 compress_size=653>\n",
      "<ZipInfo filename='test.csv.7z' compress_type=deflate file_size=4885065 compress_size=4886553>\n",
      "<ZipInfo filename='train.csv.7z' compress_type=deflate file_size=474092593 compress_size=474237203>\n",
      "<ZipInfo filename='transactions.csv.7z' compress_type=deflate file_size=219499 compress_size=219569>\n"
     ]
    }
   ],
   "source": [
    "# Check contents of compressed file\n",
    "with zipfile.ZipFile(PATH_RAW_DATA, 'r') as zip_ref:\n",
    "    for file in zip_ref.infolist():\n",
    "        print(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the contents are also compressed. Will start with the largest one and generate a parquet file with it and do ensure everything works smoothly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Unzip master file\n",
    "with zipfile.ZipFile(PATH_RAW_DATA, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items.csv.7z - 14315\n",
      "transactions.csv.7z - 219499\n",
      "holidays_events.csv.7z - 1898\n",
      ".DS_Store - 6148\n",
      "train.csv.7z - 474092593\n",
      "stores.csv.7z - 648\n",
      "oil.csv.7z - 3762\n",
      "test.csv.7z - 4885065\n",
      "sample_submission.csv.7z - 666528\n"
     ]
    }
   ],
   "source": [
    "# Get file names of interim\n",
    "file_list = os.listdir(PATH_INTERIM_DATA_DIR)\n",
    "for file in file_list:\n",
    "    print(file + ' - ' + str(os.stat(PATH_INTERIM_DATA_DIR.joinpath(file)).st_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train.csv.7z is clearly the largest file.\n",
    "stores.csv7z is the smallest.\n",
    "\n",
    "Will start small with the stores, then work with train."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viability of parquet and Pandas API with small dataset (Stores data).\n",
    "Stores is the smallest dataset. Will look at working with Pandas API and parquet files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Unzip stores\n",
    "PATH_INTERIM_DATA_STORES = PATH_INTERIM_DATA_DIR.joinpath('stores.csv.7z')\n",
    "\n",
    "with py7zr.SevenZipFile(PATH_INTERIM_DATA_STORES, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('stores.csv' in os.listdir(PATH_INTERIM_DATA_DIR))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can see stores.csv is in the folder.\n",
    "\n",
    "Now we will convert into a parquet file using Spark."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show top 5 rows\n",
    "df = spark.read.csv(str(PATH_INTERIM_DATA_DIR.joinpath('stores.csv')), header=True)\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update schema\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Schema just defaults everything to string which is not useful. Will need to clarify what the schema will be. This is be the case for all datasets most likely."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "# Need to clarify the schema\n",
    "stores_schema = StructType()\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('city', StringType(), True)\\\n",
    "    .add('state', StringType(), True)\\\n",
    "    .add('type', StringType(), True)\\\n",
    "    .add('cluster', StringType(), True) # is an integer, but given it's a grouping better to keep it categorical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rather than cast everything in place, we will just read in the file again\n",
    "df = spark.read.csv(str(PATH_INTERIM_DATA_DIR.joinpath('stores.csv')), header=True, schema=stores_schema)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write parquet file\n",
    "PATH_STORES_PQ = str(PATH_INTERIM_DATA_DIR.joinpath('stores.parquet'))\n",
    "df.write.parquet(PATH_STORES_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "|        6|        Quito|           Pichincha|   D|     13|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in from parquet\n",
    "df = spark.read.parquet(PATH_STORES_PQ)\n",
    "df.printSchema()\n",
    "df.show(6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Schema looks good. We could also partition this file but wont have to since it is so small."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pandas API with stores data\n",
    "Given pandas is so commonly used with Python, the pandas api on spark will be used moving forward.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "data": {
      "text/plain": "   store_nbr           city                           state type cluster\n0          1          Quito                       Pichincha    D      13\n1          2          Quito                       Pichincha    D      13\n2          3          Quito                       Pichincha    D       8\n3          4          Quito                       Pichincha    D       9\n4          5  Santo Domingo  Santo Domingo de los Tsachilas    D       4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_nbr</th>\n      <th>city</th>\n      <th>state</th>\n      <th>type</th>\n      <th>cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Santo Domingo</td>\n      <td>Santo Domingo de los Tsachilas</td>\n      <td>D</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "psdf = df.pandas_api()\n",
    "psdf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "       store_nbr\ncount  54.000000\nmean   27.500000\nstd    15.732133\nmin     1.000000\n25%    14.000000\n50%    27.000000\n75%    41.000000\nmax    54.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_nbr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>54.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>27.500000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>15.732133</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>27.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>41.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>54.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above looks good and works well.\n",
    "\n",
    "Below we will do the same for the largest file (train.csv) and make sure to partition it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viability of parquet and Pandas API with large data (train data).\n",
    "The train data is large so will likely be a bit fiddly in terms of memory.\n",
    "The below cells will attempt to convert the train data to a parquet file and"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Unzip train\n",
    "# Note: This is a bit duplicated at the moment, but will clean it up in the main etl script\n",
    "PATH_INTERIM_DATA_TRAIN = PATH_INTERIM_DATA_DIR.joinpath('train.csv.7z')\n",
    "\n",
    "with py7zr.SevenZipFile(PATH_INTERIM_DATA_TRAIN, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df = spark.read.csv(str(PATH_INTERIM_DATA_DIR.joinpath('train.csv')), header=True)\n",
    "\n",
    "print(\"%s seconds\" % round((time.time() - start_time), 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------+----------+-----------+\n",
      "| id|      date|store_nbr|item_nbr|unit_sales|onpromotion|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "|  0|2013-01-01|       25|  103665|       7.0|       null|\n",
      "|  1|2013-01-01|       25|  105574|       1.0|       null|\n",
      "|  2|2013-01-01|       25|  105575|       2.0|       null|\n",
      "|  3|2013-01-01|       25|  108079|       1.0|       null|\n",
      "|  4|2013-01-01|       25|  108701|       1.0|       null|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "# psdf = df.pandas_api()\n",
    "# psdf.head()\n",
    "# Pandas API is taking so long compared to Spark itself? head() must collect the entire dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Very quick to read in and show the head using spark without pandas api. A column view will likely be very slow given the underlying data is a csv.\n",
    "Need to understand the data a bit better so will split up into chunks and describe. Running all at once will likely run out of memory on this machine."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "psdf = df.pandas_api()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:================================================>       (33 + 5) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125497040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(len(psdf.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Takes a long time to even get the number of rows which is 125m. Will definitely need to partition this. Working with csv will take a long time. Will write as parquet for now, then do more exploration and update it.\n",
    "Basically we want to know\n",
    "- what to make the structure\n",
    "- how to partition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "PATH_TRAIN_INTERIM_PQ = str(PATH_INTERIM_DATA_DIR.joinpath('train.parquet'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.write.parquet(PATH_TRAIN_INTERIM_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Read in from parquet\n",
    "# df = spark.read.parquet(PATH_TRAIN_INTERIM_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# df.printSchema()\n",
    "# df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# psdf = df.pandas_api()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# psdf['onpromotion'].iloc[1:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After some more research can see that many are having problems with the speed of the Pandas api for spark. Will still with spark functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "PATH_TRAIN_INTERIM_PQ = str(PATH_INTERIM_DATA_DIR.joinpath('train.parquet'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(PATH_TRAIN_INTERIM_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|onpromotion|\n",
      "+-----------+\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('onpromotion').show(5)\n",
    "# so much faster!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(onpromotion=None), Row(onpromotion='False'), Row(onpromotion='True')]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('onpromotion').distinct().collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- item_nbr: integer (nullable = true)\n",
      " |-- unit_sales: float (nullable = true)\n",
      " |-- onpromotion: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"id\",df.id.cast('int'))\\\n",
    "    .withColumn(\"date\",df.date.cast('date'))\\\n",
    "    .withColumn(\"store_nbr\",df.store_nbr.cast('int'))\\\n",
    "    .withColumn(\"item_nbr\",df.item_nbr.cast('int'))\\\n",
    "    .withColumn(\"unit_sales\",df.unit_sales.cast('float'))\\\n",
    "    .withColumn(\"onpromotion\",df.onpromotion.cast('boolean'))\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ID column looks to be useless and just a row number we can probably remove."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=125497035), Row(id=125497036), Row(id=125497037), Row(id=125497038), Row(id=125497039)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "125497040"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.select(df.id).take(5))\n",
    "print(df.select(df.id).tail(5))\n",
    "df.count()\n",
    "# Selecting distinct ran out of memory, but can see the head and tail look like row nums."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- item_nbr: integer (nullable = true)\n",
      " |-- unit_sales: float (nullable = true)\n",
      " |-- onpromotion: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('id')\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+\n",
      "|store_nbr|onpromotion|   avg(unit_sales)|\n",
      "+---------+-----------+------------------+\n",
      "|        1|      false|3.5494276795005204|\n",
      "|        1|       true|              4.04|\n",
      "|        2|      false|3.1278493557978195|\n",
      "|        2|       true|3.6206896551724137|\n",
      "|        3|      false| 4.580110497237569|\n",
      "|        3|       true|           4.40625|\n",
      "|        4|      false| 3.128526645768025|\n",
      "|        4|       true|3.6666666666666665|\n",
      "|        5|      false| 6.432795698924731|\n",
      "|        5|       true| 7.368421052631579|\n",
      "|        6|      false| 4.289800995024875|\n",
      "|        6|       true|2.4545454545454546|\n",
      "|        7|      false| 5.587735849056604|\n",
      "|        7|       true| 6.428571428571429|\n",
      "|        8|      false| 2.788482834994463|\n",
      "|        8|       true|3.0434782608695654|\n",
      "|        9|      false| 3.294940796555436|\n",
      "|        9|       true| 3.937007874015748|\n",
      "|       11|      false| 2.387862796833773|\n",
      "|       11|       true|2.2660550458715596|\n",
      "+---------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at a single item and see if promo has an impact\n",
    "# this is just an eye-balling, proper analysis will be done during eda\n",
    "df.filter(df.item_nbr == 103665)\\\n",
    "    .filter(df.onpromotion.isNotNull())\\\n",
    "    .groupby(df.store_nbr, df.onpromotion)\\\n",
    "    .avg('unit_sales')\\\n",
    "    .sort(df.store_nbr, df.onpromotion)\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Operations seem to be possible without using the Pandas API and leveraging what PySpark has.\n",
    "Will perform some more operations on the larger dataset to ensure this will be usable.\n",
    "Saying that, it's likely we will not have to use all the data to get a decent model. But useful to be able to view it all easily during the EDA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.24 seconds\n"
     ]
    }
   ],
   "source": [
    "# Take the average of every item store and collect. See how long it takes and if it's reasonable for EDA.\n",
    "start_time = time.time()\n",
    "\n",
    "df.groupby(df.item_nbr, df.store_nbr)\\\n",
    "    .avg('unit_sales')\\\n",
    "    .collect()\n",
    "\n",
    "print(\"%s seconds\" % round((time.time() - start_time), 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 seconds for the average sales of every item-store is pretty good (given how old my computer is). This will be fine for EDA and mostly likely ok for model training.\n",
    "But as mentioned above, we will not have to use all the data for a reasonable model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Partitioning train data\n",
    "Given it is time series data, it would likely benefit by being paritioned by date. This would make it very easy to add new data, filter out old data, explore spikes, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add('date', DateType(), True)\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('item_nbr', IntegerType(), True)\\\n",
    "    .add('unit_sales', FloatType(), True)\\\n",
    "    .add('onpromotion', BooleanType(), True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Determine schemas for remaining datasets\n",
    "Will put the results into `src/data/schemas`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First decompress all remaining files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/items.csv.7z\n",
      "Decompressing: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/transactions.csv.7z\n",
      "Decompressing: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/holidays_events.csv.7z\n",
      "Decompressing: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/oil.csv.7z\n",
      "Decompressing: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/test.csv.7z\n",
      "Decompressing: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/sample_submission.csv.7z\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Get file list\n",
    "file_list = os.listdir(PATH_INTERIM_DATA_DIR)\n",
    "regex = re.compile(\".*\\\\.7z$\")\n",
    "file_list_7z = list(filter(regex.match, file_list))\n",
    "\n",
    "# Loop through file list and decompress if csv doesn't already exist\n",
    "for file in file_list_7z:\n",
    "    file_path_7z = PATH_INTERIM_DATA_DIR.joinpath(file)\n",
    "    file_path_csv = Path(re.sub(\"\\\\.7z$\", \"\", str(file_path_7z)))\n",
    "    if not file_path_csv.exists():\n",
    "        print(\"Decompressing: \" + str(file_path_7z))\n",
    "        with py7zr.SevenZipFile(file_path_7z, 'r') as zip_ref:\n",
    "            zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loop through each csv file and take a look at the head.\n",
    "This will give us a good idea of how to define the schema when writing back as parquet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def read_csv_print_head(file_path: str, n_rows: int = 3):\n",
    "\n",
    "    if not Path(file_path).exists():\n",
    "        print(file_path + \" does not exist\")\n",
    "        return;\n",
    "\n",
    "    # Read in csv file\n",
    "    df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "    # Print top n rows\n",
    "    print(\"file: \" + file_path)\n",
    "    df.show(n_rows)\n",
    "    return;\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/test.csv\n",
      "+---------+----------+---------+--------+-----------+\n",
      "|       id|      date|store_nbr|item_nbr|onpromotion|\n",
      "+---------+----------+---------+--------+-----------+\n",
      "|125497040|2017-08-16|        1|   96995|      False|\n",
      "|125497041|2017-08-16|        1|   99197|      False|\n",
      "|125497042|2017-08-16|        1|  103501|      False|\n",
      "+---------+----------+---------+--------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "file: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/train.csv\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "| id|      date|store_nbr|item_nbr|unit_sales|onpromotion|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "|  0|2013-01-01|       25|  103665|       7.0|       null|\n",
      "|  1|2013-01-01|       25|  105574|       1.0|       null|\n",
      "|  2|2013-01-01|       25|  105575|       2.0|       null|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "file: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/transactions.csv\n",
      "+----------+---------+------------+\n",
      "|      date|store_nbr|transactions|\n",
      "+----------+---------+------------+\n",
      "|2013-01-01|       25|         770|\n",
      "|2013-01-02|        1|        2111|\n",
      "|2013-01-02|        2|        2358|\n",
      "+----------+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "file: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/items.csv\n",
      "+--------+---------+-----+----------+\n",
      "|item_nbr|   family|class|perishable|\n",
      "+--------+---------+-----+----------+\n",
      "|   96995|GROCERY I| 1093|         0|\n",
      "|   99197|GROCERY I| 1067|         0|\n",
      "|  103501| CLEANING| 3008|         0|\n",
      "+--------+---------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "file: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/oil.csv\n",
      "+----------+----------+\n",
      "|      date|dcoilwtico|\n",
      "+----------+----------+\n",
      "|2013-01-01|      null|\n",
      "|2013-01-02|     93.14|\n",
      "|2013-01-03|     92.97|\n",
      "+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "file: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/holidays_events.csv\n",
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "|      date|   type|  locale|locale_name|         description|transferred|\n",
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "|2012-03-02|Holiday|   Local|      Manta|  Fundacion de Manta|      False|\n",
      "|2012-04-01|Holiday|Regional|   Cotopaxi|Provincializacion...|      False|\n",
      "|2012-04-12|Holiday|   Local|     Cuenca| Fundacion de Cuenca|      False|\n",
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "file: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/sample_submission.csv\n",
      "+---------+----------+\n",
      "|       id|unit_sales|\n",
      "+---------+----------+\n",
      "|125497040|         0|\n",
      "|125497041|         0|\n",
      "|125497042|         0|\n",
      "+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "file: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/stores.csv\n",
      "+---------+-----+---------+----+-------+\n",
      "|store_nbr| city|    state|type|cluster|\n",
      "+---------+-----+---------+----+-------+\n",
      "|        1|Quito|Pichincha|   D|     13|\n",
      "|        2|Quito|Pichincha|   D|     13|\n",
      "|        3|Quito|Pichincha|   D|      8|\n",
      "+---------+-----+---------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get file list\n",
    "file_list = os.listdir(PATH_INTERIM_DATA_DIR)\n",
    "regex = re.compile(\".*\\\\.csv$\")\n",
    "file_list_csv = list(filter(regex.match, file_list))\n",
    "\n",
    "for file in file_list_csv:\n",
    "    file_path = str(PATH_INTERIM_DATA_DIR.joinpath(file))\n",
    "    read_csv_print_head(file_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this we can make reasonable assumptions about the structure of the data.\n",
    "There may be instances where we need to adjust in the future, but this can be done if needed.\n",
    "\n",
    "Table structures will be stored in `src/data/schemas`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paritioning\n",
    "Should we partition any of the datasets?\n",
    "Can see that train and test are quite large and both are time series. In industry, we would likely partition by date since it would make it so easy to add new data as it is generated. But this is not a use case for this project as we will not have any new data coming in. Therefore we can consider partitioning in a way that would make modelling much easier. We don't know how the modelling will look at the momemt, we may end up modelling by product cluster, or store, or week.\n",
    "\n",
    "For the moment, no paritioning will be done. We may generate a feature store which can possibly benefit from paritioning, but this will be done after EDA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up\n",
    "Cleaning up all the files used in this notebook.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/items.csv.7z\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/transactions.csv.7z\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/holidays_events.csv.7z\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/train.parquet\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/stores.parquet\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/train.csv.7z\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/test.csv\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/stores.csv.7z\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/oil.csv.7z\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/train.csv\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/transactions.csv\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/items.csv\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/oil.csv\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/holidays_events.csv\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/sample_submission.csv\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/stores.csv\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/test.csv.7z\n",
      "Deleting: /Users/Patrick/PycharmProjects/supermarketDemandForecasting/data/interim/sample_submission.csv.7z\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from shutil import rmtree\n",
    "import re\n",
    "\n",
    "file_list = os.listdir(PATH_INTERIM_DATA_DIR)\n",
    "regex = re.compile(\"(.*\\\\.csv$|.*\\\\.7z$|.*\\\\.parquet$)\")\n",
    "file_list_all = list(filter(regex.match, file_list))\n",
    "\n",
    "for file in file_list_all:\n",
    "    file_path = PATH_INTERIM_DATA_DIR.joinpath(file)\n",
    "    print(\"Deleting: \" + str(file_path))\n",
    "    if file_path.is_dir():\n",
    "        rmtree(file_path)\n",
    "    else:\n",
    "        os.remove(file_path)\n",
    "\n",
    "print(\"Done.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}