{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook will investigate the viability of transforming the data into parquet files using Spark.\n",
    "# Pros with using Parquet over csv are that it has an embedded schema, takes up less storage and can extract\n",
    "#  individual columns much faster\n",
    "from config import proj\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "import py7zr # need for decompression of 7z\n",
    "\n",
    "PATH_RAW_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('raw')\n",
    "PATH_INTERIM_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('interim')\n",
    "PATH_PROC_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "favorita-grocery-sales-forecasting.zip\n"
     ]
    }
   ],
   "source": [
    "# Get compressed file name\n",
    "print(os.listdir(PATH_RAW_DATA_DIR)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "PATH_RAW_DATA = PATH_RAW_DATA_DIR.joinpath('favorita-grocery-sales-forecasting.zip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ZipInfo filename='holidays_events.csv.7z' compress_type=deflate file_size=1898 compress_size=1903>\n",
      "<ZipInfo filename='items.csv.7z' compress_type=deflate file_size=14315 compress_size=14320>\n",
      "<ZipInfo filename='oil.csv.7z' compress_type=deflate file_size=3762 compress_size=3767>\n",
      "<ZipInfo filename='sample_submission.csv.7z' compress_type=deflate file_size=666528 compress_size=649511>\n",
      "<ZipInfo filename='stores.csv.7z' compress_type=deflate file_size=648 compress_size=653>\n",
      "<ZipInfo filename='test.csv.7z' compress_type=deflate file_size=4885065 compress_size=4886553>\n",
      "<ZipInfo filename='train.csv.7z' compress_type=deflate file_size=474092593 compress_size=474237203>\n",
      "<ZipInfo filename='transactions.csv.7z' compress_type=deflate file_size=219499 compress_size=219569>\n"
     ]
    }
   ],
   "source": [
    "# Check contents of compressed file\n",
    "with zipfile.ZipFile(PATH_RAW_DATA, 'r') as zip_ref:\n",
    "    for file in zip_ref.infolist():\n",
    "        print(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the contents are also compressed. Will start with the largest one and generate a parquet file with it and do ensure everything works smoothly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Unzip master file\n",
    "with zipfile.ZipFile(PATH_RAW_DATA, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items.csv.7z - 14315\n",
      "transactions.csv.7z - 219499\n",
      "holidays_events.csv.7z - 1898\n",
      "train.csv.7z - 474092593\n",
      "stores.csv.7z - 648\n",
      "oil.csv.7z - 3762\n",
      "test.csv.7z - 4885065\n",
      "sample_submission.csv.7z - 666528\n"
     ]
    }
   ],
   "source": [
    "# Get file names of interim\n",
    "file_list = os.listdir(PATH_INTERIM_DATA_DIR)\n",
    "for file in file_list:\n",
    "    print(file + ' - ' + str(os.stat(PATH_INTERIM_DATA_DIR.joinpath(file)).st_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train.csv.7z is clearly the largest file.\n",
    "stores.csv7z is the smallest.\n",
    "\n",
    "Will start small with the stores, then work with train."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Unzip stores\n",
    "PATH_INTERIM_DATA_STORES = PATH_INTERIM_DATA_DIR.joinpath('stores.csv.7z')\n",
    "\n",
    "with py7zr.SevenZipFile(PATH_INTERIM_DATA_STORES, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('stores.csv' in os.listdir(PATH_INTERIM_DATA_DIR))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can see stores.csv is in the folder.\n",
    "\n",
    "Now we will convert into a parquet file using Spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show top 5 rows\n",
    "df = spark.read.csv(str(PATH_INTERIM_DATA_DIR.joinpath('stores.csv')), header=True)\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update schema\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "# Need to clarify the schema\n",
    "stores_schema = StructType()\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('city', StringType(), True)\\\n",
    "    .add('state', StringType(), True)\\\n",
    "    .add('type', StringType(), True)\\\n",
    "    .add('cluster', StringType(), True) # is an integer, but given it's a grouping better to keep it categorical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rather than cast everything in place, we will just read in the file again\n",
    "df = spark.read.csv(str(PATH_INTERIM_DATA_DIR.joinpath('stores.csv')), header=True, schema=stores_schema)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# Write parquet file\n",
    "PATH_STORES_PQ = str(PATH_PROC_DATA_DIR.joinpath('stores.parquet'))\n",
    "df.write.parquet(PATH_STORES_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "|        6|        Quito|           Pichincha|   D|     13|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in from parquet\n",
    "df = spark.read.parquet(PATH_STORES_PQ)\n",
    "df.printSchema()\n",
    "df.show(6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Schema looks good. We could also partition this file but wont have to since it is so small.\n",
    "\n",
    "Given pandas is so commonly used with Python, the pandas api on spark will be used moving forward."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "   store_nbr           city                           state type cluster\n0          1          Quito                       Pichincha    D      13\n1          2          Quito                       Pichincha    D      13\n2          3          Quito                       Pichincha    D       8\n3          4          Quito                       Pichincha    D       9\n4          5  Santo Domingo  Santo Domingo de los Tsachilas    D       4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_nbr</th>\n      <th>city</th>\n      <th>state</th>\n      <th>type</th>\n      <th>cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Santo Domingo</td>\n      <td>Santo Domingo de los Tsachilas</td>\n      <td>D</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "psdf = df.pandas_api()\n",
    "psdf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "       store_nbr\ncount  54.000000\nmean   27.500000\nstd    15.732133\nmin     1.000000\n25%    14.000000\n50%    27.000000\n75%    41.000000\nmax    54.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_nbr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>54.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>27.500000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>15.732133</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>27.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>41.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>54.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above looks good.\n",
    "\n",
    "Below we will do the same for the largest file (train.csv) and make sure to partition it.\n",
    "# TODO generate parquet file for train, attempt some operations on it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}