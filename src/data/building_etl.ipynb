{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploring how ETL will be done.\n",
    "This notebook will investigate the viability of transforming the data into parquet files using Spark.\n",
    "Pros with using Parquet over csv are that it has an embedded schema, takes up less storage and can extract individual columns much faster.\n",
    "\n",
    "Note that **this notebook is not an EDA**. The goal is for ETL design only therefore EDA is out of scope.\n",
    "\n",
    "This notebook will: see what data we have, understand the schemas and determine how to move forward with the ETL script."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discover what datasets we have.\n",
    "Start by defining some folder paths and looking at the downloaded data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "import py7zr # need for decompression of 7z\n",
    "import time # for timing execution\n",
    "\n",
    "PATH_RAW_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('raw')\n",
    "PATH_INTERIM_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('interim')\n",
    "PATH_PROC_DATA_DIR = Path(proj.proj_paths[\"top\"]).joinpath('data').joinpath('processed')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "favorita-grocery-sales-forecasting.zip\n"
     ]
    }
   ],
   "source": [
    "# Get compressed file name\n",
    "print(os.listdir(PATH_RAW_DATA_DIR)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "PATH_RAW_DATA = PATH_RAW_DATA_DIR.joinpath('favorita-grocery-sales-forecasting.zip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ZipInfo filename='holidays_events.csv.7z' compress_type=deflate file_size=1898 compress_size=1903>\n",
      "<ZipInfo filename='items.csv.7z' compress_type=deflate file_size=14315 compress_size=14320>\n",
      "<ZipInfo filename='oil.csv.7z' compress_type=deflate file_size=3762 compress_size=3767>\n",
      "<ZipInfo filename='sample_submission.csv.7z' compress_type=deflate file_size=666528 compress_size=649511>\n",
      "<ZipInfo filename='stores.csv.7z' compress_type=deflate file_size=648 compress_size=653>\n",
      "<ZipInfo filename='test.csv.7z' compress_type=deflate file_size=4885065 compress_size=4886553>\n",
      "<ZipInfo filename='train.csv.7z' compress_type=deflate file_size=474092593 compress_size=474237203>\n",
      "<ZipInfo filename='transactions.csv.7z' compress_type=deflate file_size=219499 compress_size=219569>\n"
     ]
    }
   ],
   "source": [
    "# Check contents of compressed file\n",
    "with zipfile.ZipFile(PATH_RAW_DATA, 'r') as zip_ref:\n",
    "    for file in zip_ref.infolist():\n",
    "        print(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the contents are also compressed. Will start with the largest one and generate a parquet file with it and do ensure everything works smoothly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Unzip master file\n",
    "with zipfile.ZipFile(PATH_RAW_DATA, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items.csv.7z - 14315\n",
      "transactions.csv.7z - 219499\n",
      "holidays_events.csv.7z - 1898\n",
      "train.csv.7z - 474092593\n",
      "stores.csv.7z - 648\n",
      "oil.csv.7z - 3762\n",
      "test.csv.7z - 4885065\n",
      "sample_submission.csv.7z - 666528\n"
     ]
    }
   ],
   "source": [
    "# Get file names of interim\n",
    "file_list = os.listdir(PATH_INTERIM_DATA_DIR)\n",
    "for file in file_list:\n",
    "    print(file + ' - ' + str(os.stat(PATH_INTERIM_DATA_DIR.joinpath(file)).st_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train.csv.7z is clearly the largest file.\n",
    "stores.csv7z is the smallest.\n",
    "\n",
    "Will start small with the stores, then work with train."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viability of parquet and Pandas API with small dataset (Stores data).\n",
    "Stores is the smallest dataset. Will look at working with Pandas API and parquet files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Unzip stores\n",
    "PATH_INTERIM_DATA_STORES = PATH_INTERIM_DATA_DIR.joinpath('stores.csv.7z')\n",
    "\n",
    "with py7zr.SevenZipFile(PATH_INTERIM_DATA_STORES, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('stores.csv' in os.listdir(PATH_INTERIM_DATA_DIR))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can see stores.csv is in the folder.\n",
    "\n",
    "Now we will convert into a parquet file using Spark."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show top 5 rows\n",
    "df = spark.read.csv(str(PATH_INTERIM_DATA_DIR.joinpath('stores.csv')), header=True)\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update schema\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Schema just defaults everything to string which is not useful. Will need to clarify what the schema will be. This is be the case for all datasets most likely."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "# Need to clarify the schema\n",
    "stores_schema = StructType()\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('city', StringType(), True)\\\n",
    "    .add('state', StringType(), True)\\\n",
    "    .add('type', StringType(), True)\\\n",
    "    .add('cluster', StringType(), True) # is an integer, but given it's a grouping better to keep it categorical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rather than cast everything in place, we will just read in the file again\n",
    "df = spark.read.csv(str(PATH_INTERIM_DATA_DIR.joinpath('stores.csv')), header=True, schema=stores_schema)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# Write parquet file\n",
    "PATH_STORES_PQ = str(PATH_PROC_DATA_DIR.joinpath('stores.parquet'))\n",
    "df.write.parquet(PATH_STORES_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "|        6|        Quito|           Pichincha|   D|     13|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in from parquet\n",
    "df = spark.read.parquet(PATH_STORES_PQ)\n",
    "df.printSchema()\n",
    "df.show(6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Schema looks good. We could also partition this file but wont have to since it is so small."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pandas API with stores data\n",
    "Given pandas is so commonly used with Python, the pandas api on spark will be used moving forward.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "   store_nbr           city                           state type cluster\n0          1          Quito                       Pichincha    D      13\n1          2          Quito                       Pichincha    D      13\n2          3          Quito                       Pichincha    D       8\n3          4          Quito                       Pichincha    D       9\n4          5  Santo Domingo  Santo Domingo de los Tsachilas    D       4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_nbr</th>\n      <th>city</th>\n      <th>state</th>\n      <th>type</th>\n      <th>cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Santo Domingo</td>\n      <td>Santo Domingo de los Tsachilas</td>\n      <td>D</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "psdf = df.pandas_api()\n",
    "psdf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "       store_nbr\ncount  54.000000\nmean   27.500000\nstd    15.732133\nmin     1.000000\n25%    14.000000\n50%    27.000000\n75%    41.000000\nmax    54.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_nbr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>54.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>27.500000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>15.732133</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>27.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>41.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>54.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above looks good and works well.\n",
    "\n",
    "Below we will do the same for the largest file (train.csv) and make sure to partition it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viability of parquet and Pandas API with large data (train data).\n",
    "The train data is large so will likely be a bit fiddly in terms of memory.\n",
    "The below cells will attempt to convert the train data to a parquet file and"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Unzip train\n",
    "# Note: This is a bit duplicated at the moment, but will clean it up in the main etl script\n",
    "PATH_INTERIM_DATA_TRAIN = PATH_INTERIM_DATA_DIR.joinpath('train.csv.7z')\n",
    "\n",
    "with py7zr.SevenZipFile(PATH_INTERIM_DATA_TRAIN, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PATH_INTERIM_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.03 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df = spark.read.csv(str(PATH_INTERIM_DATA_DIR.joinpath('train.csv')), header=True)\n",
    "\n",
    "print(\"%s seconds\" % round((time.time() - start_time), 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------+----------+-----------+\n",
      "| id|      date|store_nbr|item_nbr|unit_sales|onpromotion|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "|  0|2013-01-01|       25|  103665|       7.0|       null|\n",
      "|  1|2013-01-01|       25|  105574|       1.0|       null|\n",
      "|  2|2013-01-01|       25|  105575|       2.0|       null|\n",
      "|  3|2013-01-01|       25|  108079|       1.0|       null|\n",
      "|  4|2013-01-01|       25|  108701|       1.0|       null|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# psdf = df.pandas_api()\n",
    "# psdf.head()\n",
    "# Pandas API is taking so long compared to Spark itself? head() must collect the entire dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Very quick to read in and show the head using spark without pandas api. A column view will likely be very slow given the underlying data is a csv.\n",
    "Need to understand the data a bit better so will split up into chunks and describe. Running all at once will likely run out of memory on this machine."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "psdf = df.pandas_api()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:================================================>       (33 + 5) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125497040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(len(psdf.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Takes a long time to even get the number of rows which is 125m. Will definitely need to partition this. Working with csv will take a long time. Will write as parquet for now, then do more exploration and update it.\n",
    "Basically we want to know\n",
    "- what to make the structure\n",
    "- how to partition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "PATH_TRAIN_INTERIM_PQ = str(PATH_INTERIM_DATA_DIR.joinpath('train.parquet'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.write.parquet(PATH_TRAIN_INTERIM_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Read in from parquet\n",
    "df = spark.read.parquet(PATH_TRAIN_INTERIM_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- store_nbr: string (nullable = true)\n",
      " |-- item_nbr: string (nullable = true)\n",
      " |-- unit_sales: string (nullable = true)\n",
      " |-- onpromotion: string (nullable = true)\n",
      "\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "| id|      date|store_nbr|item_nbr|unit_sales|onpromotion|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "|  0|2013-01-01|       25|  103665|       7.0|       null|\n",
      "|  1|2013-01-01|       25|  105574|       1.0|       null|\n",
      "|  2|2013-01-01|       25|  105575|       2.0|       null|\n",
      "|  3|2013-01-01|       25|  108079|       1.0|       null|\n",
      "|  4|2013-01-01|       25|  108701|       1.0|       null|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# psdf = df.pandas_api()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# psdf['onpromotion'].iloc[1:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After some more research can see that many are having problems with the speed of the Pandas api for spark. Will still with spark functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "PATH_TRAIN_INTERIM_PQ = str(PATH_INTERIM_DATA_DIR.joinpath('train.parquet'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(PATH_TRAIN_INTERIM_PQ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|onpromotion|\n",
      "+-----------+\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('onpromotion').show(5)\n",
    "# so much faster!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(onpromotion=None), Row(onpromotion='False'), Row(onpromotion='True')]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('onpromotion').distinct().collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- item_nbr: integer (nullable = true)\n",
      " |-- unit_sales: float (nullable = true)\n",
      " |-- onpromotion: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"id\",df.id.cast('int'))\\\n",
    "    .withColumn(\"date\",df.date.cast('date'))\\\n",
    "    .withColumn(\"store_nbr\",df.store_nbr.cast('int'))\\\n",
    "    .withColumn(\"item_nbr\",df.item_nbr.cast('int'))\\\n",
    "    .withColumn(\"unit_sales\",df.unit_sales.cast('float'))\\\n",
    "    .withColumn(\"onpromotion\",df.onpromotion.cast('boolean'))\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ID column looks to be useless and just a row number we can probably remove."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id='0'), Row(id='1'), Row(id='2'), Row(id='3'), Row(id='4')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id='125497035'), Row(id='125497036'), Row(id='125497037'), Row(id='125497038'), Row(id='125497039')]\n"
     ]
    },
    {
     "data": {
      "text/plain": "125497040"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.select(df.id).take(5))\n",
    "print(df.select(df.id).tail(5))\n",
    "df.count()\n",
    "# Selecting distinct ran out of memory, but can see the head and tail look like row nums."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- store_nbr: string (nullable = true)\n",
      " |-- item_nbr: string (nullable = true)\n",
      " |-- unit_sales: string (nullable = true)\n",
      " |-- onpromotion: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('id')\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===========>                                             (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+\n",
      "|store_nbr|onpromotion|   avg(unit_sales)|\n",
      "+---------+-----------+------------------+\n",
      "|        1|      false|3.5494276795005204|\n",
      "|        1|       true|              4.04|\n",
      "|        2|      false|3.1278493557978195|\n",
      "|        2|       true|3.6206896551724137|\n",
      "|        3|      false| 4.580110497237569|\n",
      "|        3|       true|           4.40625|\n",
      "|        4|      false| 3.128526645768025|\n",
      "|        4|       true|3.6666666666666665|\n",
      "|        5|      false| 6.432795698924731|\n",
      "|        5|       true| 7.368421052631579|\n",
      "|        6|      false| 4.289800995024875|\n",
      "|        6|       true|2.4545454545454546|\n",
      "|        7|      false| 5.587735849056604|\n",
      "|        7|       true| 6.428571428571429|\n",
      "|        8|      false| 2.788482834994463|\n",
      "|        8|       true|3.0434782608695654|\n",
      "|        9|      false| 3.294940796555436|\n",
      "|        9|       true| 3.937007874015748|\n",
      "|       11|      false| 2.387862796833773|\n",
      "|       11|       true|2.2660550458715596|\n",
      "+---------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Look at a single item and see if promo has an impact\n",
    "# this is just an eye-balling, proper analysis will be done during eda\n",
    "df.filter(df.item_nbr == 103665)\\\n",
    "    .filter(df.onpromotion.isNotNull())\\\n",
    "    .groupby(df.store_nbr, df.onpromotion)\\\n",
    "    .avg('unit_sales')\\\n",
    "    .sort(df.store_nbr, df.onpromotion)\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Operations seem to be possible without using the Pandas API and leveraging what PySpark has.\n",
    "Will perform some more operations on the larger dataset to ensure this will be usable.\n",
    "Saying that, it's likely we will not have to use all the data to get a decent model. But useful to be able to view it all easily during the EDA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Take the average of every item store and collect. See how long it takes and if it's reasonable for EDA.\n",
    "start_time = time.time()\n",
    "\n",
    "df.groupby(df.item_nbr, df.store_nbr)\\\n",
    "    .avg('unit_sales')\\\n",
    "    .collect()\n",
    "\n",
    "print(\"%s seconds\" % round((time.time() - start_time), 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 seconds for the average sales of every item-store is pretty good (given how old my computer is). This will be fine for EDA and mostly likely ok for model training.\n",
    "But as mentioned above, we will not have to use all the data for a reasonable model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Partitioning train data\n",
    "Given it is time series data, it would likely benefit by being paritioned by date. This would make it very easy to add new data, filter out old data, explore spikes, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train schema"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, FloatType, BooleanType\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add('date', DateType(), True)\\\n",
    "    .add('store_nbr', IntegerType(), True)\\\n",
    "    .add('item_nbr', IntegerType(), True)\\\n",
    "    .add('unit_sales', FloatType(), True)\\\n",
    "    .add('onpromotion', BooleanType(), True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Determine schemas for remaining datasets\n",
    "Will put the results into `src/data/schemas`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}