{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Base Feature/Future Store\n",
    "The resulting output will be a feature store (if based on train) and a future store (if based on test) that is at an item-loc-day level that includes all item, store and event information.\n",
    "\n",
    "These feature stores will be known as the *BASE FEATURE/FUTURE STORE*. Further information will be added to these feature stores but the base will remain relatively static.\n",
    "For example, a pipeline that is attempting to better model sales during events will engineer features in the pipeline that add to the base feature store, rather than it getting built ito the base feature store here.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "import pyspark.sql.functions as sf\n",
    "from src.utils.validation_utils import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "build_on = \"test\" # train builds the feature store, test builds the future store."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add provided data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pull in train or test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if build_on == \"train\":\n",
    "    base = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"train.parquet\")))\n",
    "elif build_on == \"test\":\n",
    "    base = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"test.parquet\")))\n",
    "else:\n",
    "    raise NotImplemented(\"Can only build feature or future store\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+--------+-----------+------------+-----+----------+\n",
      "|       id|      date|store_nbr|item_nbr|onpromotion|      family|class|perishable|\n",
      "+---------+----------+---------+--------+-----------+------------+-----+----------+\n",
      "|125497040|2017-08-16|        1|   96995|      false|   GROCERY I| 1093|         0|\n",
      "|125497041|2017-08-16|        1|   99197|      false|   GROCERY I| 1067|         0|\n",
      "|125497042|2017-08-16|        1|  103501|      false|    CLEANING| 3008|         0|\n",
      "|125497043|2017-08-16|        1|  103520|      false|   GROCERY I| 1028|         0|\n",
      "|125497044|2017-08-16|        1|  103665|      false|BREAD/BAKERY| 2712|         1|\n",
      "+---------+----------+---------+--------+-----------+------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"items.parquet\")))\n",
    "base = base.join(items, base.item_nbr == items.item_nbr, \"left\").drop(items.item_nbr)\n",
    "base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add store"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+--------+-----------+------------+-----+----------+-----+---------+----+-------+\n",
      "|       id|      date|store_nbr|item_nbr|onpromotion|      family|class|perishable| city|    state|type|cluster|\n",
      "+---------+----------+---------+--------+-----------+------------+-----+----------+-----+---------+----+-------+\n",
      "|125497040|2017-08-16|        1|   96995|      false|   GROCERY I| 1093|         0|Quito|Pichincha|   D|     13|\n",
      "|125497041|2017-08-16|        1|   99197|      false|   GROCERY I| 1067|         0|Quito|Pichincha|   D|     13|\n",
      "|125497042|2017-08-16|        1|  103501|      false|    CLEANING| 3008|         0|Quito|Pichincha|   D|     13|\n",
      "|125497043|2017-08-16|        1|  103520|      false|   GROCERY I| 1028|         0|Quito|Pichincha|   D|     13|\n",
      "|125497044|2017-08-16|        1|  103665|      false|BREAD/BAKERY| 2712|         1|Quito|Pichincha|   D|     13|\n",
      "+---------+----------+---------+--------+-----------+------------+-----+----------+-----+---------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stores = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"stores.parquet\")))\n",
    "base = base.join(stores, base.store_nbr == stores.store_nbr, \"left\").drop(stores.store_nbr)\n",
    "base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New and cleared item flag\n",
    "This flag will help us know how to treat particular items. If they have been cleared they wont need to be predicted for, so we can possibly filter them out. Or if they are new, a different treatment will need to be applied since the model wont have seen these items before."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"train.parquet\")))\n",
    "test = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"test.parquet\")))\n",
    "\n",
    "train_items = train.select(\"item_nbr\", sf.lit(1).alias(\"train_fl\")).distinct()\n",
    "test_items = test.select(\"item_nbr\", sf.lit(1).alias(\"test_fl\")).distinct()\n",
    "\n",
    "item_coverage = train_items.join(test_items, train_items.item_nbr == test_items.item_nbr, \"full\")\n",
    "\n",
    "new_items = item_coverage.filter(\"train_fl is null\")\\\n",
    "    .drop(train_items.item_nbr)\\\n",
    "    .select(test_items.item_nbr)\\\n",
    "    .withColumn(\"new_item\", sf.lit(True)) # items not in train\n",
    "\n",
    "cleared_items = item_coverage.filter(\"test_fl is null\")\\\n",
    "    .drop(test_items.item_nbr)\\\n",
    "    .select(train_items.item_nbr)\\\n",
    "    .withColumn(\"cleared_item\", sf.lit(True)) # items not in test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "base = base\\\n",
    "    .join(new_items, [\"item_nbr\"], \"left\")\\\n",
    "    .join(cleared_items, [\"item_nbr\"], \"left\")\\\n",
    "    .na.fill(value = False, subset=[\"new_item\", \"cleared_item\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add events\n",
    "Engineer appropriate flags for events.\n",
    "This will add the following columns to the base set\n",
    "- event_nat - flag for national event\n",
    "- event_reg - flag for regional event\n",
    "- event_loc - flag for local event\n",
    "- event_type - type of event"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "holidays = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"holidays_events.parquet\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "holidays_nat = holidays\\\n",
    "    .filter(\"locale == 'National'\")\\\n",
    "    .filter(\"transferred == false\")\\\n",
    "    .select([\"date\", \"type\"])\\\n",
    "    .withColumnRenamed(\"type\", \"type_nat\")\\\n",
    "    .withColumn(\"event_nat\", sf.lit(True))\n",
    "holidays_reg = holidays\\\n",
    "    .filter(\"locale == 'Regional'\")\\\n",
    "    .filter(\"transferred == false\")\\\n",
    "    .select([\"date\", \"type\", \"locale_name\"])\\\n",
    "    .withColumnRenamed(\"locale_name\", \"state\")\\\n",
    "    .withColumnRenamed(\"type\", \"type_reg\")\\\n",
    "    .withColumn(\"event_reg\", sf.lit(True))\n",
    "holidays_loc = holidays\\\n",
    "    .filter(\"locale == 'Local'\")\\\n",
    "    .filter(\"transferred == false\")\\\n",
    "    .select([\"date\", \"type\", \"locale_name\"])\\\n",
    "    .withColumnRenamed(\"locale_name\", \"city\")\\\n",
    "    .withColumnRenamed(\"type\", \"type_loc\")\\\n",
    "    .withColumn(\"event_loc\", sf.lit(True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "base = base\\\n",
    "    .join(holidays_nat, [\"date\"], \"left\")\\\n",
    "    .join(holidays_reg, [\"date\", \"state\"], \"left\")\\\n",
    "    .join(holidays_loc, [\"date\", \"city\"], \"left\")\\\n",
    "    .na.fill(value = False, subset = [\"event_nat\", \"event_reg\", \"event_loc\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "base = base.withColumn(\"event_type\", sf.coalesce(base[\"type_nat\"], base[\"type_reg\"], base[\"type_loc\"]))\\\n",
    "    .drop(\"type_nat\", \"type_reg\", \"type_loc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation\n",
    "- Count rows\n",
    "- Check for nulls, etc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check row counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if build_on == \"train\":\n",
    "    assert base.count() == 125497040, \"Feature store rows have changed due to bad join. Should be the same as train.\"\n",
    "elif build_on == \"test\":\n",
    "    assert base.count() == 3370464, \"Future store rows have changed due to bad join. Should be the same as test.\"\n",
    "else:\n",
    "    raise NotImplemented(\"Can only build feature or future store\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check for missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_allowed = [\"event_type\"]\n",
    "null_not_allowed = [col_name for col_name in base.columns if col_name not in null_allowed]\n",
    "\n",
    "for col_name in null_not_allowed:\n",
    "    assert_col_has_no_null(base, col_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write feature store\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}