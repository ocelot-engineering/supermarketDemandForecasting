{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Feature/Future Store\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "build_on = \"test\" # train builds the feature store, test builds the future store."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add provided data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pull in train or test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if build_on == \"train\":\n",
    "    base = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"train.parquet\")))\n",
    "elif build_on == \"test\":\n",
    "    base = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"test.parquet\")))\n",
    "else:\n",
    "    raise NotImplemented(\"Can only build feature or future store\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+--------+-----------+------------+-----+----------+\n",
      "|       id|      date|store_nbr|item_nbr|onpromotion|      family|class|perishable|\n",
      "+---------+----------+---------+--------+-----------+------------+-----+----------+\n",
      "|125497040|2017-08-16|        1|   96995|      false|   GROCERY I| 1093|         0|\n",
      "|125497041|2017-08-16|        1|   99197|      false|   GROCERY I| 1067|         0|\n",
      "|125497042|2017-08-16|        1|  103501|      false|    CLEANING| 3008|         0|\n",
      "|125497043|2017-08-16|        1|  103520|      false|   GROCERY I| 1028|         0|\n",
      "|125497044|2017-08-16|        1|  103665|      false|BREAD/BAKERY| 2712|         1|\n",
      "+---------+----------+---------+--------+-----------+------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"items.parquet\")))\n",
    "base = base.join(items, base.item_nbr == items.item_nbr, \"left\").drop(items.item_nbr)\n",
    "base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add store"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+--------+-----------+------------+-----+----------+-----+---------+----+-------+\n",
      "|       id|      date|store_nbr|item_nbr|onpromotion|      family|class|perishable| city|    state|type|cluster|\n",
      "+---------+----------+---------+--------+-----------+------------+-----+----------+-----+---------+----+-------+\n",
      "|125497040|2017-08-16|        1|   96995|      false|   GROCERY I| 1093|         0|Quito|Pichincha|   D|     13|\n",
      "|125497041|2017-08-16|        1|   99197|      false|   GROCERY I| 1067|         0|Quito|Pichincha|   D|     13|\n",
      "|125497042|2017-08-16|        1|  103501|      false|    CLEANING| 3008|         0|Quito|Pichincha|   D|     13|\n",
      "|125497043|2017-08-16|        1|  103520|      false|   GROCERY I| 1028|         0|Quito|Pichincha|   D|     13|\n",
      "|125497044|2017-08-16|        1|  103665|      false|BREAD/BAKERY| 2712|         1|Quito|Pichincha|   D|     13|\n",
      "+---------+----------+---------+--------+-----------+------------+-----+----------+-----+---------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stores = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"stores.parquet\")))\n",
    "base = base.join(stores, base.store_nbr == stores.store_nbr, \"left\").drop(stores.store_nbr)\n",
    "base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New and cleared item flag\n",
    "This flag will help us know how to treat particular items. If they have been cleared they wont need to be predicted for, so we can possibly filter them out. Or if they are new, a different treatment will need to be applied since the model wont have seen these items before."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"train.parquet\")))\n",
    "test = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"test.parquet\")))\n",
    "\n",
    "train_items = train.select(\"item_nbr\", sf.lit(1).alias(\"train_fl\")).distinct()\n",
    "test_items = test.select(\"item_nbr\", sf.lit(1).alias(\"test_fl\")).distinct()\n",
    "\n",
    "item_coverage = train_items.join(test_items, train_items.item_nbr == test_items.item_nbr, \"full\")\n",
    "\n",
    "new_items = item_coverage.filter(\"train_fl is null\")\\\n",
    "    .drop(train_items.item_nbr)\\\n",
    "    .select(test_items.item_nbr)\\\n",
    "    .withColumn(\"new_item\", sf.lit(1)) # items not in train\n",
    "\n",
    "cleared_items = item_coverage.filter(\"test_fl is null\")\\\n",
    "    .drop(test_items.item_nbr)\\\n",
    "    .select(train_items.item_nbr)\\\n",
    "    .withColumn(\"cleared_item\", sf.lit(1)) # items not in test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "base = base\\\n",
    "    .join(new_items, [\"item_nbr\"], \"left\")\\\n",
    "    .join(cleared_items, [\"item_nbr\"], \"left\")\\\n",
    "    .na.fill(value = 0, subset=[\"new_item\", \"cleared_item\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add events\n",
    "TODO need to engineer appropriate flags"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation\n",
    "- Count rows\n",
    "- Check for nulls, etc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 125497040 - train\n",
    "# 3370464 - test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# base.filter(\"new_item is null\").count()\n",
    "# base.filter(\"cleared_item is null\").count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+---------+-----------+------------+-----+----------+-----+---------+----+-------+--------+------------+\n",
      "|item_nbr|       id|      date|store_nbr|onpromotion|      family|class|perishable| city|    state|type|cluster|new_item|cleared_item|\n",
      "+--------+---------+----------+---------+-----------+------------+-----+----------+-----+---------+----+-------+--------+------------+\n",
      "|   96995|125497040|2017-08-16|        1|      false|   GROCERY I| 1093|         0|Quito|Pichincha|   D|     13|       0|           0|\n",
      "|   99197|125497041|2017-08-16|        1|      false|   GROCERY I| 1067|         0|Quito|Pichincha|   D|     13|       0|           0|\n",
      "|  103501|125497042|2017-08-16|        1|      false|    CLEANING| 3008|         0|Quito|Pichincha|   D|     13|       0|           0|\n",
      "|  103520|125497043|2017-08-16|        1|      false|   GROCERY I| 1028|         0|Quito|Pichincha|   D|     13|       0|           0|\n",
      "|  103665|125497044|2017-08-16|        1|      false|BREAD/BAKERY| 2712|         1|Quito|Pichincha|   D|     13|       0|           0|\n",
      "+--------+---------+----------+---------+-----------+------------+-----+----------+-----+---------+----+-------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}