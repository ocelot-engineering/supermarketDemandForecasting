{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Base Feature/Future Store\n",
    "The resulting output will be a feature store (if based on train) and a future store (if based on test) that is at an item-loc-day level that includes all item, store and event information.\n",
    "\n",
    "These feature stores will be known as the *BASE FEATURE/FUTURE STORE*. Further information will be added to these feature stores but the base will remain relatively static.\n",
    "For example, a pipeline that is attempting to better model sales during events will engineer features in the pipeline that add to the base feature store, rather than it getting built ito the base feature store here.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import proj\n",
    "import pyspark.sql.functions as sf\n",
    "from src.utils.validation_utils import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "build_on = \"train\" # train builds the feature store, test builds the future store."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add provided data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pull in train or test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if build_on == \"train\":\n",
    "    base = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"train.parquet\")))\n",
    "    base = base.filter(\"date >= '2015-08-14'\") ## Filter to two most recent years\n",
    "elif build_on == \"test\":\n",
    "    base = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"test.parquet\")))\n",
    "else:\n",
    "    raise NotImplemented(\"Can only build feature or future store\")\n",
    "orig_row_count = base.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "items = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"items.parquet\")))\n",
    "base = base.join(items, base.item_nbr == items.item_nbr, \"left\").drop(items.item_nbr)\n",
    "# base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add store"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "stores = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"stores.parquet\")))\n",
    "base = base.join(stores, base.store_nbr == stores.store_nbr, \"left\").drop(stores.store_nbr)\n",
    "# base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New and cleared item flag\n",
    "This flag will help us know how to treat particular items. If they have been cleared they wont need to be predicted for, so we can possibly filter them out. Or if they are new, a different treatment will need to be applied since the model wont have seen these items before.\n",
    "\n",
    "*Note:* new item does not corrospond to date, i.e on `2016-05-01` `new_item==True` doesn't mean item was new on `2016-05-01`, instead if means the item is not in the train dataset, but is included in the test set. This very specific for modelling a single date range. Also, it doesn't make sense to see any `new_item==True` in train or any `cleared_item==True` in test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"train.parquet\")))\n",
    "test = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"test.parquet\")))\n",
    "\n",
    "train_items = train.select(\"item_nbr\", sf.lit(1).alias(\"train_fl\")).distinct()\n",
    "test_items = test.select(\"item_nbr\", sf.lit(1).alias(\"test_fl\")).distinct()\n",
    "\n",
    "item_coverage = train_items.join(test_items, train_items.item_nbr == test_items.item_nbr, \"full\")\n",
    "\n",
    "new_items = item_coverage.filter(\"train_fl is null\")\\\n",
    "    .drop(train_items.item_nbr)\\\n",
    "    .select(test_items.item_nbr)\\\n",
    "    .withColumn(\"new_item\", sf.lit(True)) # items not in train\n",
    "\n",
    "cleared_items = item_coverage.filter(\"test_fl is null\")\\\n",
    "    .drop(test_items.item_nbr)\\\n",
    "    .select(train_items.item_nbr)\\\n",
    "    .withColumn(\"cleared_item\", sf.lit(True)) # items not in test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "base = base\\\n",
    "    .join(new_items, [\"item_nbr\"], \"left\")\\\n",
    "    .join(cleared_items, [\"item_nbr\"], \"left\")\\\n",
    "    .na.fill(value = False, subset=[\"new_item\", \"cleared_item\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add events\n",
    "Engineer appropriate flags for events.\n",
    "This will add the following columns to the base set\n",
    "- event_nat - flag for national event\n",
    "- event_reg - flag for regional event\n",
    "- event_loc - flag for local event\n",
    "- event_type - type of event"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "holidays = spark.read.parquet(str(proj.Config.paths.get(\"data_proc\").joinpath(\"holidays_events.parquet\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# deal with dupes and filter transferred and work days\n",
    "holidays = holidays\\\n",
    "    .filter(\"transferred == false\")\\\n",
    "    .filter(\"type != 'Work Day'\")\\\n",
    "    .groupby([\"date\", \"locale\", \"locale_name\"])\\\n",
    "    .agg(sf.concat_ws(\"|\", sf.collect_list(\"type\")).alias(\"type\")) # combines multiple types into a single row"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "holidays_nat = holidays\\\n",
    "    .filter(\"locale == 'National'\")\\\n",
    "    .select([\"date\", \"type\"])\\\n",
    "    .withColumnRenamed(\"type\", \"type_nat\")\\\n",
    "    .withColumn(\"event_nat\", sf.lit(True))\n",
    "\n",
    "holidays_reg = holidays\\\n",
    "    .filter(\"locale == 'Regional'\")\\\n",
    "    .select([\"date\", \"type\", \"locale_name\"])\\\n",
    "    .withColumnRenamed(\"locale_name\", \"state\")\\\n",
    "    .withColumnRenamed(\"type\", \"type_reg\")\\\n",
    "    .withColumn(\"event_reg\", sf.lit(True))\n",
    "\n",
    "holidays_loc = holidays\\\n",
    "    .filter(\"locale == 'Local'\")\\\n",
    "    .select([\"date\", \"type\", \"locale_name\"])\\\n",
    "    .withColumnRenamed(\"locale_name\", \"city\")\\\n",
    "    .withColumnRenamed(\"type\", \"type_loc\")\\\n",
    "    .withColumn(\"event_loc\", sf.lit(True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# join all event flags to base\n",
    "base = base\\\n",
    "    .join(holidays_nat, [\"date\"], \"left\")\\\n",
    "    .join(holidays_reg, [\"date\", \"state\"], \"left\")\\\n",
    "    .join(holidays_loc, [\"date\", \"city\"], \"left\")\\\n",
    "    .na.fill(value = False, subset = [\"event_nat\", \"event_reg\", \"event_loc\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# take first non-null event type and create event flag\n",
    "base = base\\\n",
    "    .withColumn(\"event_type\", sf.coalesce(base[\"type_nat\"], base[\"type_reg\"], base[\"type_loc\"]))\\\n",
    "    .drop(\"type_nat\", \"type_reg\", \"type_loc\")\\\n",
    "    .withColumn(\"event\", sf.greatest(\"event_nat\", \"event_reg\", \"event_reg\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Times series"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- day of week\n",
    "- weekday\n",
    "- weekend\n",
    "- etc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other\n",
    "- avg/median item sales\n",
    "- avg/median item sales week day\n",
    "- avg/median item sales week end"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation\n",
    "- Count rows\n",
    "- Check for nulls, etc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check row counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert base.count() == orig_row_count, \"Feature/Future store rows have changed due to bad join. Should be the same as original row count.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check for missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_allowed = [\"event_type\"]\n",
    "null_not_allowed = [col_name for col_name in base.columns if col_name not in null_allowed]\n",
    "\n",
    "for col_name in null_not_allowed:\n",
    "    assert_col_has_no_null(base, col_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+--------+---------+---------+-----------+------------+-----+----------+----+-------+--------+------------+---------+---------+---------+----------+\n",
      "|      date| city|    state|item_nbr|       id|store_nbr|onpromotion|      family|class|perishable|type|cluster|new_item|cleared_item|event_nat|event_reg|event_loc|event_type|\n",
      "+----------+-----+---------+--------+---------+---------+-----------+------------+-----+----------+----+-------+--------+------------+---------+---------+---------+----------+\n",
      "|2017-08-16|Quito|Pichincha|   96995|125497040|        1|      false|   GROCERY I| 1093|         0|   D|     13|   false|       false|    false|    false|    false|      null|\n",
      "|2017-08-16|Quito|Pichincha|   99197|125497041|        1|      false|   GROCERY I| 1067|         0|   D|     13|   false|       false|    false|    false|    false|      null|\n",
      "|2017-08-16|Quito|Pichincha|  103501|125497042|        1|      false|    CLEANING| 3008|         0|   D|     13|   false|       false|    false|    false|    false|      null|\n",
      "|2017-08-16|Quito|Pichincha|  103520|125497043|        1|      false|   GROCERY I| 1028|         0|   D|     13|   false|       false|    false|    false|    false|      null|\n",
      "|2017-08-16|Quito|Pichincha|  103665|125497044|        1|      false|BREAD/BAKERY| 2712|         1|   D|     13|   false|       false|    false|    false|    false|      null|\n",
      "+----------+-----+---------+--------+---------+---------+-----------+------------+-----+----------+----+-------+--------+------------+---------+---------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write feature store\n",
    "\n",
    "Initial thoughts about modelling are that it will be done at a family-cluster level, that will produce 33*5=165 smaller models, that will then feed into models that apportion out the sales. This depends on how the promos work since they will have a significant impact on sales."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if build_on == \"train\":\n",
    "    file_name = \"feature_store.parquet\"\n",
    "elif build_on == \"test\":\n",
    "    file_name = \"future_store.parquet\"\n",
    "else:\n",
    "    raise NotImplemented(\"Can only build feature or future store\")\n",
    "\n",
    "write_path = proj.Config.paths.get(\"data_proc\").joinpath(file_name)\n",
    "base.write.partitionBy(\"family\").parquet(str(write_path), mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}